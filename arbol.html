<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ãrbol de decisiÃ³n</title>
    <!-- Estilos -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/arbol.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
</head>
  </head>
<body>
    <div class="container">
        <!-- PropÃ³sito de la herramienta -->
        <section id="tool-purpose" class="mb-4">
            <div class="rounded-container p-3">
                <button class="close-button" aria-label="Cerrar">&times;</button>
                <h2 id="page-title" class="mb-0"></h2>
                <p>
                  El Ã¡rbol de decisiÃ³n es una herramienta interactiva diseÃ±ada para ayudar a los actores del ecosistema de inteligencia artificial (IA) a identificar, categorizar y gestionar riesgos asociados con sesgos de gÃ©nero en las soluciones de IA. A partir de preguntas clave y respuestas especÃ­ficas, el Ã¡rbol guÃ­a a los usuarios a travÃ©s de un proceso lÃ³gico y estructurado para tomar decisiones informadas, implementar acciones preventivas y utilizar herramientas adecuadas.
                </p>
                <p>
                    Los sesgos identificados han sido clasificados segÃºn la TaxonomÃ­a de Riesgos de IA del MIT, permitiendo su categorizaciÃ³n dentro de un marco reconocido internacionalmente. Esta clasificaciÃ³n facilita una mejor comprensiÃ³n de los riesgos y su impacto en el desarrollo de IA mÃ¡s equitativa y responsable. Para mÃ¡s informaciÃ³n, visita: <a href="https://airisk.mit.edu/" target="_blank">MIT AI Risk Repository</a>.
                </p>
            </div>
        </section>

    </div>
    
<div class="markmap" id="mindmap">
    <script type="text/template">

      ---
      title: Ãrbol de decisiÃ³n
      markmap:
        colorFreezeLevel: 3
        embedAssets: true
        maxWidth: 300
        initialExpandLevel: 2
        
      ---
      
      # ğŸŒ³ Ãrbol de DecisiÃ³n
      
      ## ğŸ–¥ï¸ğŸ¤– Â¿Desea desarrollar una soluciÃ³n basada en IA? Â¿En quÃ© fase se encuentra?
      
      ### 1ï¸âƒ£ ğŸ“œ PlanificaciÃ³n y diseÃ±o  
      - Â¿Las hipÃ³tesis iniciales o las suposiciones del proyecto han sido revisadas crÃ­ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿El diseÃ±o del sistema considera cÃ³mo los usuarios podrÃ­an interactuar de manera sesgada hacia ciertos gÃ©neros, y se han implementado medidas para mitigar estos patrones?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 02.08.00  
            **Entidad:** Humano  
            **IntenciÃ³n:** No intencional  
            **Momento:** Pre-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
               - Â¿El diseÃ±o de la interfaz o sistema contempla las necesidades y experiencias de todos los gÃ©neros, asegurando una experiencia inclusiva y equitativa?  
                 <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                 **ID Riesgo:** 05.01.00  
                 **Entidad:** IA  
                 **IntenciÃ³n:** No intencional  
                 **Momento:** Post-implementaciÃ³n  
                 **Tema:** DiscriminaciÃ³n y toxicidad  
                 **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                 - SÃ­  
                   - Â¿El sistema es igualmente accesible y Ãºtil para personas de todos los gÃ©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
                     <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                     **ID Riesgo:** 05.01.00  
                     **Entidad:** IA  
                     **IntenciÃ³n:** No intencional  
                     **Momento:** Post-implementaciÃ³n  
                     **Tema:** DiscriminaciÃ³n y toxicidad  
                     **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                     - SÃ­  
                       - Â¿Se han considerado las realidades de gÃ©nero del contexto en el que se aplica la soluciÃ³n, asegurando que los resultados sean equitativos?  
                         <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                         **ID Riesgo:** 05.01.00  
                         **Entidad:** IA  
                         **IntenciÃ³n:** No intencional  
                         **Momento:** Post-implementaciÃ³n  
                         **Tema:** DiscriminaciÃ³n y toxicidad  
                         **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n 
      
                         - SÃ­  
                           - Â¿El modelo aborda adecuadamente las particularidades de gÃ©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos gÃ©neros?  
                             <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                             **ID Riesgo:** 05.01.00  
                             **Entidad:** IA  
                             **IntenciÃ³n:** No intencional  
                             **Momento:** Post-implementaciÃ³n  
                             **Tema:** DiscriminaciÃ³n y toxicidad  
                             **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n 
      
                             - SÃ­  
                               - Â¿El equipo de desarrollo es diverso en tÃ©rminos de gÃ©nero y perspectiva?  
                                 <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                 **ID Riesgo:** 01.01.00  
                                 **Entidad:** Humano  
                                 **IntenciÃ³n:** No intencional  
                                 **Momento:** Otros  
                                 **Tema:** SocioeconÃ³mico y ambiental  
                                 **Subtema:** Falla de gobernanza  
      
                                  - SÃ­  
                                    - Â¿El propÃ³sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos gÃ©neros?  
                                      <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                      **ID Riesgo:** 05.01.00  
                                      **Entidad:** IA  
                                      **IntenciÃ³n:** No intencional  
                                      **Momento:** Post-implementaciÃ³n  
                                      **Tema:** DiscriminaciÃ³n y toxicidad  
                                      **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                      - SÃ­  
                                        - Â¿Se han definido responsabilidades claras en el equipo para prevenir sesgos de gÃ©nero?  
                                          <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                          **ID Riesgo:** 01.01.00  
                                          **Entidad:** Humano  
                                          **IntenciÃ³n:** No intencional  
                                          **Momento:** Otros  
                                          **Tema:** SocioeconÃ³mico y ambiental  
                                          **Subtema:** Falla de gobernanza  
      
                                          - SÃ­
                                            - âœ… **Riesgo Bajo** ğŸŸ¢
                                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.
                                           <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                                           [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
                                           [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                                           [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
                                           [Aequitas](https://github.com/dssg/aequitas)
                                           [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)
                                           [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)
                                           [UserTesting](https://www-usertesting-com.translate.goog/)
      
                                          - No
                                            - ğŸš¨ **Riesgo Alto ğŸ”´ 
                                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda establecer mecanismos de rendiciÃ³n de cuentas, definir roles de responsabilidad en el equipo y aplicar auditorÃ­as internas y externas sobre equidad de gÃ©nero.  
                                            <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                                            [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
                                            [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                                            
                                      - No  
                                        - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda involucrar a expertos en gÃ©nero, comunidades locales y otros actores relevantes para identificar necesidades especÃ­ficas y cÃ³mo el sistema puede impactar a diferentes gÃ©neros en el contexto.  
      
                                  - No
                                    - âš ï¸ **Riesgo Medio** ğŸŸ   
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como medio. Se recomienda fomentar la diversidad en la conformaciÃ³n del equipo de desarrollo, implementar polÃ­ticas de inclusiÃ³n y realizar auditorÃ­as de equidad para evitar sesgos en el diseÃ±o y desarrollo de la IA.  
       
                             - No
                               - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cÃ³mo el modelo aplica sus predicciones en el dominio especÃ­fico, ajustar parÃ¡metros segÃºn las necesidades.  
       
                         - No
                           - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de gÃ©nero del contexto. Para ello, evalÃºe la representatividad de los datos, aplique tÃ©cnicas de mitigaciÃ³n de sesgos y valide el impacto en distintos grupos.  
       
                     - No 
                       - ğŸš¨ **Riesgo Alto** ğŸ”´  
                          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditorÃ­as de accesibilidad desde una perspectiva de gÃ©nero, implementar mejoras en el diseÃ±o y garantizar estÃ¡ndares de accesibilidad universal.  
                          <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                          [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  
      
                 - No 
                   - ğŸš¨ **Riesgo Alto** ğŸ”´  
                      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el diseÃ±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de gÃ©nero y ajustar elementos de diseÃ±o para garantizar neutralidad.  
                      <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                      [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)
                      [UserTesting](https://www-usertesting-com.translate.goog/)  
      
            - No 
               - ğŸš¨ **Riesgo Alto** ğŸ”´  
                  <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables.  
                  <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                  [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  
      
        - No
           - ğŸš¨ **Riesgo Alto** ğŸ”´  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar crÃ­ticamente las hipÃ³tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
              <br>ğŸ› ï¸ **Herramientas recomendadas:** 
              [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom)
              [UserTesting](https://www-usertesting-com.translate.goog/)  
        
      
      ### 2ï¸âƒ£ ğŸ“‚ RecopilaciÃ³n y tratamiento de datos  
      - Â¿Los datos utilizados reflejan de manera equitativa y proporcional a todos los gÃ©neros, incluidos aquellos histÃ³ricamente subrepresentados o con identidades diversas?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿El proceso de selecciÃ³n de datos evita privilegiar estereotipos o caracterÃ­sticas especÃ­ficas asociadas a ciertos gÃ©neros, y se basa en criterios inclusivos y objetivos?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 02.01.01  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Otros  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿Se han identificado y mitigado patrones o roles de gÃ©nero perpetuados en los datos que podrÃ­an trasladar desigualdades pasadas al modelo?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 02.08.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Pre-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­  
                  - Â¿Se han revisado los datos para garantizar que no excluyan representaciones importantes de gÃ©neros diversos o minoritarios?  
                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                    **ID Riesgo:** 02.08.02  
                    **Entidad:** IA  
                    **IntenciÃ³n:** No intencional  
                    **Momento:** Pre-implementaciÃ³n  
                    **Tema:** DiscriminaciÃ³n y toxicidad  
                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                    - SÃ­  
                      - Â¿Las hipÃ³tesis iniciales o las suposiciones del proyecto han sido revisadas crÃ­ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                        **ID Riesgo:** 02.08.02  
                        **Entidad:** IA  
                        **IntenciÃ³n:** No intencional  
                        **Momento:** Pre-implementaciÃ³n  
                        **Tema:** DiscriminaciÃ³n y toxicidad  
                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                        - SÃ­  
                          - Â¿Se estÃ¡n buscando y considerando activamente evidencias que desafÃ­en suposiciones previas relacionadas con gÃ©nero, en lugar de reforzar estereotipos existentes?  
                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                            **ID Riesgo:** 02.08.02  
                            **Entidad:** IA  
                            **IntenciÃ³n:** No intencional  
                            **Momento:** Pre-implementaciÃ³n  
                            **Tema:** DiscriminaciÃ³n y toxicidad  
                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                            - SÃ­  
                              - Â¿La soluciÃ³n garantiza que todas las personas, independientemente de su gÃ©nero, tengan igual acceso, participaciÃ³n y representaciÃ³n en las interacciones con el sistema?  
                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **IntenciÃ³n:** No intencional  
                                **Momento:** Post-implementaciÃ³n  
                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                - SÃ­  
                                  - Â¿Se han implementado medidas para evitar la filtraciÃ³n de datos sensibles relacionados con gÃ©nero?  
                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                    **ID Riesgo:** 11.03.00  
                                    **Entidad:** IA  
                                    **IntenciÃ³n:** No intencional  
                                    **Momento:** Otros  
                                    **Tema:** Seguridad y privacidad  
                                    **Subtema:** Fugas de datos personales  
      
                                    - SÃ­  
                                      - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
                                      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                                      <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                      [Qualtrics](https://www.qualtrics.com/es/plataforma/)  
                                      [UserTesting](https://www-usertesting-com.translate.goog/)  
                                      [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
                                      [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                                      [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                                      [Google Dataset Search](https://datasetsearch.research.google.com/help)  
                                      [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                                      [Aequitas](https://github.com/dssg/aequitas)  
        
                                    - No  
                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacciÃ³n segmentadas por gÃ©nero, identificar barreras de acceso y mejorar el diseÃ±o para garantizar una experiencia inclusiva.  
                                        <br>ğŸ› ï¸ **Herramientas recomendadas:** 
                                        [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
        
                                - No
                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacciÃ³n segmentadas por gÃ©nero, identificar barreras de acceso y mejorar el diseÃ±o para garantizar una experiencia inclusiva.  
                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                    [Qualtrics](https://www.qualtrics.com/es/plataforma/)  
                                    [UserTesting](https://www-usertesting-com.translate.goog/)  
      
                            - No  
                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda aplicar tÃ©cnicas de anonimizaciÃ³n de datos y privacidad diferencial para proteger informaciÃ³n de identidad de gÃ©nero.  
                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                                [Aequitas](https://github.com/dssg/aequitas)  
        
                        - No  
                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el anÃ¡lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad.  
                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                            [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
                            [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
       
                    - No  
                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar crÃ­ticamente las hipÃ³tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                        [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                        [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
       
                - No  
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de gÃ©nero y realizar auditorÃ­as de inclusiÃ³n.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                    [Google Dataset Search](https://datasetsearch.research.google.com/help)  
        
            - No  
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                  <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda auditar los datos para identificar tendencias histÃ³ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual.  
                  <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                  [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                  [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
        
        - No  
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar y ajustar los criterios de selecciÃ³n para garantizar neutralidad, asÃ­ como auditar los datos seleccionados para identificar patrones sesgados.  
              <br>ğŸ› ï¸ **Herramientas recomendadas:**  
              [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
            [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
       
      
      ### 3ï¸âƒ£ âš™ï¸ CreaciÃ³n de modelo(s) y/o adaptaciÃ³n de modelo(s)
      - Â¿Los datos utilizados reflejan de manera equitativa y proporcional a todos los gÃ©neros, incluidos aquellos histÃ³ricamente subrepresentados o con identidades diversas?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿El proceso de selecciÃ³n de datos evita privilegiar estereotipos o caracterÃ­sticas especÃ­ficas asociadas a ciertos gÃ©neros, y se basa en criterios inclusivos y objetivos?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 02.01.01  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Otros  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿Se han identificado y mitigado patrones o roles de gÃ©nero perpetuados en los datos que podrÃ­an trasladar desigualdades pasadas al modelo?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 02.08.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Pre-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­  
                  - Â¿Se han revisado los datos para garantizar que no excluyan representaciones importantes de gÃ©neros diversos o minoritarios?  
                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                    **ID Riesgo:** 02.08.02  
                    **Entidad:** IA  
                    **IntenciÃ³n:** No intencional  
                    **Momento:** Pre-implementaciÃ³n  
                    **Tema:** DiscriminaciÃ³n y toxicidad  
                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                    - SÃ­  
                      - Â¿El modelo ha sido evaluado en contextos especÃ­ficos relacionados con gÃ©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                        **ID Riesgo:** 05.01.00  
                        **Entidad:** IA  
                        **IntenciÃ³n:** No intencional  
                        **Momento:** Post-implementaciÃ³n  
                        **Tema:** DiscriminaciÃ³n y toxicidad  
                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                        - SÃ­  
                          - Â¿Se valida que el modelo sea aplicable a contextos con dinÃ¡micas de gÃ©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                            **ID Riesgo:** 05.01.00  
                            **Entidad:** IA  
                            **IntenciÃ³n:** No intencional  
                            **Momento:** Post-implementaciÃ³n  
                            **Tema:** DiscriminaciÃ³n y toxicidad  
                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                            - SÃ­  
                              - Â¿Se han considerado las realidades culturales y de gÃ©nero del contexto en el que se aplica la soluciÃ³n, asegurando que los resultados sean culturalmente sensibles y equitativos?  
                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **IntenciÃ³n:** No intencional  
                                **Momento:** Post-implementaciÃ³n  
                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                - SÃ­  
                                  - Â¿El modelo aborda adecuadamente las particularidades de gÃ©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos gÃ©neros?  
                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                    **ID Riesgo:** 05.01.00  
                                    **Entidad:** IA  
                                    **IntenciÃ³n:** No intencional  
                                    **Momento:** Post-implementaciÃ³n  
                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                    - SÃ­
                                      - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                        [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                                        [Aequitas](https://github.com/dssg/aequitas)  
                                        [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)  
                                        [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                                        [Google Dataset Search](https://datasetsearch.research.google.com/help)
                          
                                    - No 
                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cÃ³mo el modelo aplica sus predicciones en el dominio especÃ­fico, ajustar parÃ¡metros segÃºn las necesidades.  
       
                                - No 
                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar anÃ¡lisis cultural del modelo en el contexto especÃ­fico, colaborar con expertos locales y ajustar los resultados para reflejar las dinÃ¡micas culturales.  
                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                    [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                                    [Fairlearn](https://github.com/markmap/coc-markmap)
      
                            - No
                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar pruebas de transferencia en el nuevo contexto, ajustar los parÃ¡metros del modelo y validar que los resultados reflejen dinÃ¡micas de gÃ©nero locales.  
                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                                [Fairlearn](https://github.com/markmap/coc-markmap)  
                                [Aequitas](https://github.com/dssg/aequitas) 
      
                        - No 
                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios especÃ­ficos de gÃ©nero, ajustar parÃ¡metros para mejorar el desempeÃ±o en esos contextos y realizar validaciones periÃ³dicas.  
                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                            [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
      
                    - No 
                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de gÃ©nero y realizar auditorÃ­as de inclusiÃ³n.  
                          <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                          [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                          [Google Dataset Search](https://datasetsearch.research.google.com/help)
      
                - No 
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda auditar los datos para identificar tendencias histÃ³ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual.  
                      <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                      [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                      [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
      
            - No
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar y ajustar los criterios de selecciÃ³n para garantizar neutralidad, asÃ­ como auditar los datos seleccionados para identificar patrones sesgados.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
      
        - No 
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar un anÃ¡lisis de distribuciÃ³n de datos para identificar grupos subrepresentados, ampliar la recolecciÃ³n de datos de fuentes diversas y aplicar tÃ©cnicas de balanceo o re-pesado para corregir desbalances en los datos.  
              <br>ğŸ› ï¸ **Herramientas recomendadas:**  
              [Fairlearn](https://github.com/markmap/coc-markmap)  
              [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
              [Aequitas](https://github.com/dssg/aequitas) 
      
      ### 4ï¸âƒ£ ğŸ” Prueba, evaluaciÃ³n, verificaciÃ³n y validaciÃ³n
      - Â¿Las hipÃ³tesis iniciales o las suposiciones del proyecto han sido revisadas crÃ­ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿Se estÃ¡n buscando y considerando activamente evidencias que desafÃ­en suposiciones previas relacionadas con gÃ©nero, en lugar de reforzar estereotipos existentes?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 02.08.02  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Pre-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿Los resultados del modelo estÃ¡n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de gÃ©nero?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 05.01.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Post-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­  
                  - Â¿El diseÃ±o del sistema considera cÃ³mo los usuarios podrÃ­an interactuar de manera sesgada hacia ciertos gÃ©neros, y se han implementado medidas para mitigar estos patrones?  
                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                    **ID Riesgo:** 02.08.00  
                    **Entidad:** Humano  
                    **IntenciÃ³n:** No intencional  
                    **Momento:** Pre-implementaciÃ³n  
                    **Tema:** DiscriminaciÃ³n y toxicidad  
                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                    - SÃ­  
                      - Â¿La soluciÃ³n garantiza que todas las personas, independientemente de su gÃ©nero, tengan igual acceso, participaciÃ³n y representaciÃ³n en las interacciones con el sistema?  
                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                        **ID Riesgo:** 05.01.00  
                        **Entidad:** IA  
                        **IntenciÃ³n:** No intencional  
                        **Momento:** Post-implementaciÃ³n  
                        **Tema:** DiscriminaciÃ³n y toxicidad  
                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                        - SÃ­  
                          - Â¿El diseÃ±o de la interfaz o sistema contempla las necesidades y experiencias de todos los gÃ©neros, asegurando una experiencia inclusiva y equitativa?  
                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                            **ID Riesgo:** 05.01.00  
                            **Entidad:** IA  
                            **IntenciÃ³n:** No intencional  
                            **Momento:** Post-implementaciÃ³n  
                            **Tema:** DiscriminaciÃ³n y toxicidad  
                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                            - SÃ­  
                              - Â¿El sistema es igualmente accesible y Ãºtil para personas de todos los gÃ©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **IntenciÃ³n:** No intencional  
                                **Momento:** Post-implementaciÃ³n  
                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                - SÃ­  
                                  - Â¿El modelo ha sido evaluado en contextos especÃ­ficos relacionados con gÃ©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                    **ID Riesgo:** 05.01.00  
                                    **Entidad:** IA  
                                    **IntenciÃ³n:** No intencional  
                                    **Momento:** Post-implementaciÃ³n  
                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                    - SÃ­  
                                      - Â¿Se monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de gÃ©nero, y se aplican correcciones cuando es necesario?  
                                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                        **ID Riesgo:** 05.01.00  
                                        **Entidad:** IA  
                                        **IntenciÃ³n:** No intencional  
                                        **Momento:** Post-implementaciÃ³n  
                                        **Tema:** DiscriminaciÃ³n y toxicidad  
                                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                        - SÃ­  
                                          - Â¿El modelo evita amplificar desigualdades o estereotipos de gÃ©nero existentes en los datos o interacciones del sistema?  
                                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                            **ID Riesgo:** 05.01.00  
                                            **Entidad:** IA  
                                            **IntenciÃ³n:** No intencional  
                                            **Momento:** Post-implementaciÃ³n  
                                            **Tema:** DiscriminaciÃ³n y toxicidad  
                                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                            - SÃ­  
                                              - Â¿Las adaptaciones del sistema consideran posibles sesgos de gÃ©nero en los datos y resultados, y se ajustan para evitar perpetuar errores o desigualdades?  
                                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                **ID Riesgo:** 05.01.00  
                                                **Entidad:** IA  
                                                **IntenciÃ³n:** No intencional  
                                                **Momento:** Post-implementaciÃ³n  
                                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                - SÃ­  
                                                  - Â¿El modelo ha sido probado en contextos donde pueda reforzar desigualdades de gÃ©nero?  
                                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                    **ID Riesgo:** 05.01.00  
                                                    **Entidad:** IA  
                                                    **IntenciÃ³n:** No intencional  
                                                    **Momento:** Post-implementaciÃ³n  
                                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
        
                                                    - SÃ­ 
                                                      - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
                                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                        [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                                                        [Herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html)  
                                                        [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                                                        [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)  
                                                        [Aequitas](https://github.com/dssg/aequitas)  
                                                        [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  
                                                        [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                                                        [UserTesting](https://www-usertesting-com.translate.goog/) 
      
                                                    - No
                                                       - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el desempeÃ±o del modelo en diferentes grupos poblacionales y aplicar tÃ©cnicas de mitigaciÃ³n de sesgos.  
                                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                        [Aequitas](https://github.com/dssg/aequitas)  
                                                        [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  
      
                                                - No 
                                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los procesos de adaptaciÃ³n automÃ¡tica del sistema, establecer reglas para detectar adaptaciones sesgadas y ajustar el modelo segÃºn sea necesario.  
                                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                    [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                                                    [Herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html)
                                                     
                                            - No
                                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar las predicciones para identificar amplificaciones no deseadas, reentrenar el modelo si se detectan patrones sesgados y validar los resultados con datos ajustados.  
                                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)  
                                                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html) 
      
                                        - No 
                                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema periÃ³dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas.  
                                          <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                          [Aequitas](https://github.com/dssg/aequitas)
      
                                    - No
                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios especÃ­ficos de gÃ©nero, ajustar parÃ¡metros para mejorar el desempeÃ±o en esos contextos y realizar validaciones periÃ³dicas.  
                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                        [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/) 
      
                                - No
                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditorÃ­as de accesibilidad desde una perspectiva de gÃ©nero, implementar mejoras en el diseÃ±o y garantizar estÃ¡ndares de accesibilidad universal.  
                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                    [Aequitas](https://github.com/dssg/aequitas)  
                                    [Fairness Indicators](https://github.com/tensorflow/fairness-indicators) 
      
                            - No 
                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el diseÃ±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de gÃ©nero y ajustar elementos de diseÃ±o para garantizar neutralidad.  
                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                                [UserTesting](https://www-usertesting-com.translate.goog/)
      
                        - No
                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacciÃ³n segmentadas por gÃ©nero, identificar barreras de acceso y mejorar el diseÃ±o para garantizar una experiencia inclusiva.  
                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                            [Qualtrics](https://www.qualtrics.com/es/plataforma/)  
                            [UserTesting](https://www-usertesting-com.translate.goog/) 
      
                    - No 
                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables.  
                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                        [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom)  
                        [UserTesting](https://www-usertesting-com.translate.goog/)
      
                - No
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaciÃ³n.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Fairlearn](https://github.com/markmap/coc-markmap)  
                    [Aequitas](https://github.com/dssg/aequitas)
      
            - No 
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el anÃ¡lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
      
        - No
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar crÃ­ticamente las hipÃ³tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
            [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
      
      ### 5ï¸âƒ£ ğŸš€ Entrada en servicio/despliegue  
      - Â¿La soluciÃ³n garantiza que todas las personas, independientemente de su gÃ©nero, tengan igual acceso, participaciÃ³n y representaciÃ³n en las interacciones con el sistema?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 05.01.00  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Post-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿El diseÃ±o de la interfaz o sistema contempla las necesidades y experiencias de todos los gÃ©neros, asegurando una experiencia inclusiva y equitativa?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 05.01.00  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Post-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿Las decisiones durante la implementaciÃ³n consideran posibles impactos desiguales entre gÃ©neros, y se toman medidas para garantizar resultados equitativos?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 05.01.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Post-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­
                  - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Fairlearn](https://github.com/markmap/coc-markmap)  
                    [Aequitas](https://github.com/dssg/aequitas)  
                    [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                    [UserTesting](https://www-usertesting-com.translate.goog/) 
      
                - No
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaciÃ³n con un enfoque inclusivo, realizar auditorÃ­as tÃ©cnicas y Ã©ticas y garantizar la participaciÃ³n de expertos en gÃ©nero.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Aequitas](https://github.com/dssg/aequitas) 
            - No
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el diseÃ±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de gÃ©nero y ajustar elementos de diseÃ±o para garantizar neutralidad.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)  
                [UserTesting](https://www-usertesting-com.translate.goog/)  
      
        - No 
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacciÃ³n segmentadas por gÃ©nero, identificar barreras de acceso y mejorar el diseÃ±o para garantizar una experiencia inclusiva.  
          <br>ğŸ› ï¸ **Herramientas recomendadas:**  
          [Qualtrics](https://www.qualtrics.com/es/plataforma/)  
          [UserTesting](https://www-usertesting-com.translate.goog/) 
      
      ### 6ï¸âƒ£ ğŸ“¡ ExplotaciÃ³n y supervisiÃ³n (Monitoreo y ajuste)
      - Â¿Se monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de gÃ©nero, y se aplican correcciones cuando es necesario?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 05.01.00  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Post-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿Se han evaluado impactos imprevistos de la IA en la equidad de gÃ©nero antes de su implementaciÃ³n?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 05.01.00  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Post-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
        
            - SÃ­
              - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
              <br>ğŸ› ï¸ **Herramientas recomendadas:**  
              [Aequitas](https://github.com/dssg/aequitas) 
      
            - No
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar estudios de impacto en equidad de gÃ©nero antes de la implementaciÃ³n y evaluar cÃ³mo diferentes poblaciones interactÃºan con la IA.  
              <br>ğŸ› ï¸ **Herramientas recomendadas:**  
              [Aequitas](https://github.com/dssg/aequitas) 
      
        - No
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema periÃ³dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas.  
          <br>ğŸ› ï¸ **Herramientas recomendadas:**  
          [Aequitas](https://github.com/dssg/aequitas)  
      
      ### 7ï¸âƒ£ ğŸ—‘ï¸ Retirada/desmantelamiento  
      - Â¿La retirada del sistema afecta el acceso a ciertos gÃ©neros o grupos subrepresentados?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 05.01.00  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Post-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditorÃ­as para identificar quÃ© grupos dependen del sistema y asegurar alternativas de acceso antes de la retirada.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [Aequitas](https://github.com/dssg/aequitas)  
      
        - No  
          - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
              
      ## ğŸ’°ğŸ¤– Â¿Desea adquirir una IA? Â¿En quÃ© fase se encuentra
      
      ### 1ï¸âƒ£ ğŸ“œ PlanificaciÃ³n y anÃ¡lisis inicial  
      - Â¿Los resultados del modelo estÃ¡n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de gÃ©nero?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 05.01.00  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Post-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [Fairlearn](https://github.com/markmap/coc-markmap)  
            [Aequitas](https://github.com/dssg/aequitas)  
      
        - No  
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaciÃ³n.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [Fairlearn](https://github.com/markmap/coc-markmap)  
            [Aequitas](https://github.com/dssg/aequitas)  
      
      
      ### 2ï¸âƒ£ ğŸ” EvaluaciÃ³n y selecciÃ³n de soluciones  
      - Â¿Los datos utilizados reflejan de manera equitativa y proporcional a todos los gÃ©neros, incluidos aquellos histÃ³ricamente subrepresentados o con identidades diversas?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿El proceso de selecciÃ³n de datos evita privilegiar estereotipos o caracterÃ­sticas especÃ­ficas asociadas a ciertos gÃ©neros, y se basa en criterios inclusivos y objetivos?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 02.01.01  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Otros  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿Se han identificado y mitigado patrones o roles de gÃ©nero perpetuados en los datos que podrÃ­an trasladar desigualdades pasadas al modelo?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 02.08.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Pre-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­  
                  - Â¿Se han revisado los datos para garantizar que no excluyan representaciones importantes de gÃ©neros diversos o minoritarios?  
                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                    **ID Riesgo:** 02.08.02  
                    **Entidad:** IA  
                    **IntenciÃ³n:** No intencional  
                    **Momento:** Pre-implementaciÃ³n  
                    **Tema:** DiscriminaciÃ³n y toxicidad  
                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                    - SÃ­  
                      - Â¿Las hipÃ³tesis iniciales o las suposiciones del proyecto han sido revisadas crÃ­ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                        **ID Riesgo:** 02.08.02  
                        **Entidad:** IA  
                        **IntenciÃ³n:** No intencional  
                        **Momento:** Pre-implementaciÃ³n  
                        **Tema:** DiscriminaciÃ³n y toxicidad  
                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                        - SÃ­  
                          - Â¿Se estÃ¡n buscando y considerando activamente evidencias que desafÃ­en suposiciones previas relacionadas con gÃ©nero, en lugar de reforzar estereotipos existentes?  
                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                            **ID Riesgo:** 02.08.02  
                            **Entidad:** IA  
                            **IntenciÃ³n:** No intencional  
                            **Momento:** Pre-implementaciÃ³n  
                            **Tema:** DiscriminaciÃ³n y toxicidad  
                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                            - SÃ­  
                              - Â¿Se valida que el modelo sea aplicable a contextos con dinÃ¡micas de gÃ©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **IntenciÃ³n:** No intencional  
                                **Momento:** Post-implementaciÃ³n  
                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                - SÃ­  
                                  - Â¿Se han considerado las realidades de gÃ©nero del contexto en el que se aplica la soluciÃ³n, asegurando que los resultados sean equitativos?  
                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                    **ID Riesgo:** 05.01.00  
                                    **Entidad:** IA  
                                    **IntenciÃ³n:** No intencional  
                                    **Momento:** Post-implementaciÃ³n  
                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                    - SÃ­  
                                      - Â¿El modelo aborda adecuadamente las particularidades de gÃ©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos gÃ©neros?  
                                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                        **ID Riesgo:** 05.01.00  
                                        **Entidad:** IA  
                                        **IntenciÃ³n:** No intencional  
                                        **Momento:** Post-implementaciÃ³n  
                                        **Tema:** DiscriminaciÃ³n y toxicidad  
                                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                        - SÃ­  
                                          - Â¿El propÃ³sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos gÃ©neros?  
                                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                            **ID Riesgo:** 05.01.00  
                                            **Entidad:** IA  
                                            **IntenciÃ³n:** No intencional  
                                            **Momento:** Post-implementaciÃ³n  
                                            **Tema:** DiscriminaciÃ³n y toxicidad  
                                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                            - SÃ­  
                                              - Â¿Se han realizado auditorÃ­as sobre el contenido generado para evitar sesgos de gÃ©nero?  
                                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                **ID Riesgo:** 05.01.00  
                                                **Entidad:** IA  
                                                **IntenciÃ³n:** No intencional  
                                                **Momento:** Post-implementaciÃ³n  
                                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                - SÃ­  
                                                  - Â¿Se han implementado mecanismos de explicabilidad para identificar sesgos de gÃ©nero en la toma de decisiones?  
                                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                    **ID Riesgo:** 05.01.00  
                                                    **Entidad:** IA  
                                                    **IntenciÃ³n:** No intencional  
                                                    **Momento:** Post-implementaciÃ³n  
                                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                    - SÃ­
                                                      - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
                                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                        [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                                                        [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                                                        [Aequitas](https://github.com/dssg/aequitas)  
                                                        [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
                                                        [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                                                        [Google Dataset Search](https://datasetsearch.research.google.com/help) 
      
                                                    - No
                                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que el sistema proporcione explicaciones claras sobre sus decisiones y utilizar tÃ©cnicas de interpretabilidad para detectar sesgos ocultos.  
                                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                        [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)  
                                                        [SHAP](https://shap.readthedocs.io/en/latest/)  
                                                        [LIME](https://github.com/marcotcr/lime) 
      
                                                - No
                                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda implementar revisiones sistemÃ¡ticas del contenido generado por la IA y establecer filtros que detecten y mitiguen sesgos en el lenguaje.  
                                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                    [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/) 
      
                                            - No 
                                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda involucrar a expertos en gÃ©nero, comunidades locales y otros actores relevantes para identificar necesidades especÃ­ficas y cÃ³mo el sistema puede impactar a diferentes gÃ©neros en el contexto.  
      
                                        - No 
                                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cÃ³mo el modelo aplica sus predicciones en el dominio especÃ­fico, ajustar parÃ¡metros segÃºn las necesidades.  
      
                                    - No 
                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de gÃ©nero del contexto. Para ello, evalÃºe la representatividad de los datos, aplique tÃ©cnicas de mitigaciÃ³n de sesgos y valide el impacto en distintos grupos.  
      
                                - No
                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de gÃ©nero del contexto. Para ello, evalÃºe la representatividad de los datos, aplique tÃ©cnicas de mitigaciÃ³n de sesgos y valide el impacto en distintos grupos.  
                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                    [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/) 
      
                            - No 
                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el anÃ¡lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad.  
                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
                                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
      
                        - No 
                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar crÃ­ticamente las hipÃ³tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                            [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                            [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
      
                    - No
                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de gÃ©nero y realizar auditorÃ­as de inclusiÃ³n.  
                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                        [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
                        [Google Dataset Search](https://datasetsearch.research.google.com/help) 
      
                - No 
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda auditar los datos para identificar tendencias histÃ³ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                    [AI Fairness 360](https://aif360.readthedocs.io/en/stable/) 
      
            - No 
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar y ajustar los criterios de selecciÃ³n para garantizar neutralidad, asÃ­ como auditar los datos seleccionados para identificar patrones sesgados.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
                [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
      
        - No
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar un anÃ¡lisis de distribuciÃ³n de datos para identificar grupos subrepresentados, ampliar la recolecciÃ³n de datos de fuentes diversas y aplicar tÃ©cnicas de balanceo o re-pesado para corregir desbalances en los datos.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [Fairlearn](https://github.com/markmap/coc-markmap)  
            [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/)  
            [Aequitas](https://github.com/dssg/aequitas)
      
      ### 3ï¸âƒ£ ğŸš€ ImplementaciÃ³n e integraciÃ³n  
      - Â¿El diseÃ±o del sistema considera cÃ³mo los usuarios podrÃ­an interactuar de manera sesgada hacia ciertos gÃ©neros, y se han implementado medidas para mitigar estos patrones?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.00  
        **Entidad:** Humano  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿Las decisiones durante la implementaciÃ³n consideran posibles impactos desiguales entre gÃ©neros, y se toman medidas para garantizar resultados equitativos?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 05.01.00  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Post-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­
              - âœ… **Riesgo Bajo** ğŸŸ¢  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [Fairlearn](https://github.com/markmap/coc-markmap)
                [Aequitas](https://github.com/dssg/aequitas)  
      
            - No 
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaciÃ³n con un enfoque inclusivo, realizar auditorÃ­as tÃ©cnicas y Ã©ticas y garantizar la participaciÃ³n de expertos en gÃ©nero.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [Aequitas](https://github.com/dssg/aequitas)  
      
        - No 
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables.  
            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
            [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom)
            [UserTesting](https://www-usertesting-com.translate.goog/)   
      
      ### 4ï¸âƒ£ ğŸ“¡ ExplotaciÃ³n y supervisiÃ³n  
      - Â¿Se estÃ¡n buscando y considerando activamente evidencias que desafÃ­en suposiciones previas relacionadas con gÃ©nero, en lugar de reforzar estereotipos existentes?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 02.08.02  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Pre-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­  
          - Â¿Los resultados del modelo estÃ¡n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de gÃ©nero?  
            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
            **ID Riesgo:** 05.01.00  
            **Entidad:** IA  
            **IntenciÃ³n:** No intencional  
            **Momento:** Post-implementaciÃ³n  
            **Tema:** DiscriminaciÃ³n y toxicidad  
            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
            - SÃ­  
              - Â¿La soluciÃ³n garantiza que todas las personas, independientemente de su gÃ©nero, tengan igual acceso, participaciÃ³n y representaciÃ³n en las interacciones con el sistema?  
                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                **ID Riesgo:** 05.01.00  
                **Entidad:** IA  
                **IntenciÃ³n:** No intencional  
                **Momento:** Post-implementaciÃ³n  
                **Tema:** DiscriminaciÃ³n y toxicidad  
                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                - SÃ­  
                  - Â¿El diseÃ±o de la interfaz o sistema contempla las necesidades y experiencias de todos los gÃ©neros, asegurando una experiencia inclusiva y equitativa?  
                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                    **ID Riesgo:** 05.01.00  
                    **Entidad:** IA  
                    **IntenciÃ³n:** No intencional  
                    **Momento:** Post-implementaciÃ³n  
                    **Tema:** DiscriminaciÃ³n y toxicidad  
                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                    - SÃ­  
                      - Â¿El sistema es igualmente accesible y Ãºtil para personas de todos los gÃ©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                        **ID Riesgo:** 05.01.00  
                        **Entidad:** IA  
                        **IntenciÃ³n:** No intencional  
                        **Momento:** Post-implementaciÃ³n  
                        **Tema:** DiscriminaciÃ³n y toxicidad  
                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                        - SÃ­  
                          - Â¿El modelo ha sido evaluado en contextos especÃ­ficos relacionados con gÃ©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                            **ID Riesgo:** 05.01.00  
                            **Entidad:** IA  
                            **IntenciÃ³n:** No intencional  
                            **Momento:** Post-implementaciÃ³n  
                            **Tema:** DiscriminaciÃ³n y toxicidad  
                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                            - SÃ­  
                              - Â¿Las decisiones durante la implementaciÃ³n consideran posibles impactos desiguales entre gÃ©neros, y se toman medidas para garantizar resultados equitativos?  
                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **IntenciÃ³n:** No intencional  
                                **Momento:** Post-implementaciÃ³n  
                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                - SÃ­  
                                  - Â¿Se monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de gÃ©nero, y se aplican correcciones cuando es necesario?  
                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                    **ID Riesgo:** 05.01.00  
                                    **Entidad:** IA  
                                    **IntenciÃ³n:** No intencional  
                                    **Momento:** Post-implementaciÃ³n  
                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                    - SÃ­  
                                      - Â¿El modelo evita amplificar desigualdades o estereotipos de gÃ©nero existentes en los datos o interacciones del sistema?  
                                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                        **ID Riesgo:** 05.01.00  
                                        **Entidad:** IA  
                                        **IntenciÃ³n:** No intencional  
                                        **Momento:** Post-implementaciÃ³n  
                                        **Tema:** DiscriminaciÃ³n y toxicidad  
                                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                        - SÃ­  
                                          - Â¿Las adaptaciones del sistema consideran posibles sesgos de gÃ©nero en los datos y resultados, y se ajustan para evitar perpetuar errores o desigualdades?  
                                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                            **ID Riesgo:** 05.01.00  
                                            **Entidad:** IA  
                                            **IntenciÃ³n:** No intencional  
                                            **Momento:** Post-implementaciÃ³n  
                                            **Tema:** DiscriminaciÃ³n y toxicidad  
                                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                            - SÃ­  
                                              - Â¿Se valida que el modelo sea aplicable a contextos con dinÃ¡micas de gÃ©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                                                <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                **ID Riesgo:** 05.01.00  
                                                **Entidad:** IA  
                                                **IntenciÃ³n:** No intencional  
                                                **Momento:** Post-implementaciÃ³n  
                                                **Tema:** DiscriminaciÃ³n y toxicidad  
                                                **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                - SÃ­  
                                                  - Â¿Se han considerado las realidades de gÃ©nero del contexto en el que se aplica la soluciÃ³n, asegurando que los resultados sean equitativos?  
                                                    <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                    **ID Riesgo:** 05.01.00  
                                                    **Entidad:** IA  
                                                    **IntenciÃ³n:** No intencional  
                                                    **Momento:** Post-implementaciÃ³n  
                                                    **Tema:** DiscriminaciÃ³n y toxicidad  
                                                    **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                    - SÃ­  
                                                      - Â¿El modelo aborda adecuadamente las particularidades de gÃ©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos gÃ©neros?  
                                                        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                        **ID Riesgo:** 05.01.00  
                                                        **Entidad:** IA  
                                                        **IntenciÃ³n:** No intencional  
                                                        **Momento:** Post-implementaciÃ³n  
                                                        **Tema:** DiscriminaciÃ³n y toxicidad  
                                                        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                        - SÃ­  
                                                          - Â¿El propÃ³sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos gÃ©neros?  
                                                            <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
                                                            **ID Riesgo:** 05.01.00  
                                                            **Entidad:** IA  
                                                            **IntenciÃ³n:** No intencional  
                                                            **Momento:** Post-implementaciÃ³n  
                                                            **Tema:** DiscriminaciÃ³n y toxicidad  
                                                            **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
                                                            - SÃ­                     
                                                              - âœ… **Riesgo Bajo** ğŸŸ¢  
                                                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.  
                                                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                                [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
                                                                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                                                                [Aequitas](https://github.com/dssg/aequitas)
                                                                [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)
                                                                [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html)
                                                                [MLOps dashboards](https://github.com/SumoLogic-Labs/MLOPs-Dashboards)
                                                                [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)  
      
                                                            - No 
                                                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda involucrar a expertos en gÃ©nero, comunidades locales y otros actores relevantes para identificar necesidades especÃ­ficas y cÃ³mo el sistema puede impactar a diferentes gÃ©neros en el contexto.  
      
                                                        - No
                                                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cÃ³mo el modelo aplica sus predicciones en el dominio especÃ­fico, ajustar parÃ¡metros segÃºn las necesidades.  
      
                                                    - No
                                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de gÃ©nero del contexto. Para ello, evalÃºe la representatividad de los datos, aplique tÃ©cnicas de mitigaciÃ³n de sesgos y valide el impacto en distintos grupos.  
      
                                                - No 
                                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar pruebas de transferencia en el nuevo contexto, ajustar los parÃ¡metros del modelo y validar que los resultados reflejen dinÃ¡micas de gÃ©nero locales.  
      
                                            - No 
                                              - ğŸš¨ **Riesgo Alto** ğŸ”´
                                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los procesos de adaptaciÃ³n automÃ¡tica del sistema, establecer reglas para detectar adaptaciones sesgadas y ajustar el modelo segÃºn sea necesario.  
                                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                                [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc)
                                                [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html)  
      
                                        - No 
                                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar las predicciones para identificar amplificaciones no deseadas, reentrenar el modelo si se detectan patrones sesgados y validar los resultados con datos ajustados.  
                                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                            [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
                                            [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
      
                                    - No 
                                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema periÃ³dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas.  
                                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                        [Aequitas](https://github.com/dssg/aequitas) 
      
                                - No
                                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaciÃ³n con un enfoque inclusivo, realizar auditorÃ­as tÃ©cnicas y Ã©ticas y garantizar la participaciÃ³n de expertos en gÃ©nero.  
                                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                    [Aequitas](https://github.com/dssg/aequitas)
      
                            - No
                              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios especÃ­ficos de gÃ©nero, ajustar parÃ¡metros para mejorar el desempeÃ±o en esos contextos y realizar validaciones periÃ³dicas.  
                                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                                [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
      
                        - No 
                          - ğŸš¨ **Riesgo Alto** ğŸ”´  
                            <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditorÃ­as de accesibilidad desde una perspectiva de gÃ©nero, implementar mejoras en el diseÃ±o y garantizar estÃ¡ndares de accesibilidad universal.  
                            <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                            [Aequitas](https://github.com/dssg/aequitas)
                            [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  
      
                    - No 
                      - ğŸš¨ **Riesgo Alto** ğŸ”´  
                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el diseÃ±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de gÃ©nero y ajustar elementos de diseÃ±o para garantizar neutralidad.  
                        <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                        [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)  
      
                - No 
                  - ğŸš¨ **Riesgo Alto** ğŸ”´  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacciÃ³n segmentadas por gÃ©nero, identificar barreras de acceso y mejorar el diseÃ±o para garantizar una experiencia inclusiva.  
                    <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                    [Qualtrics](https://www.qualtrics.com/es/plataforma/)
                    [UserTesting](https://www-usertesting-com.translate.goog/)  
      
            - No  
              - ğŸš¨ **Riesgo Alto** ğŸ”´  
                <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaciÃ³n.  
                <br>ğŸ› ï¸ **Herramientas recomendadas:**  
                [Fairlearn](https://github.com/markmap/coc-markmap)
                [Aequitas](https://github.com/dssg/aequitas)    
      
        - No
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
            De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el anÃ¡lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad.  
            ğŸ› ï¸ **Herramientas recomendadas:** 
            [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)
      
      
      ### 5ï¸âƒ£ ğŸ—‘ï¸ Retirada o reemplazo de la soluciÃ³n  
      - Â¿La retirada del sistema afecta el acceso a ciertos gÃ©neros o grupos subrepresentados?  
        <br>ğŸ”– **ClasificaciÃ³n riesgo MIT**  
        **ID Riesgo:** 05.01.00  
        **Entidad:** IA  
        **IntenciÃ³n:** No intencional  
        **Momento:** Post-implementaciÃ³n  
        **Tema:** DiscriminaciÃ³n y toxicidad  
        **Subtema:** DiscriminaciÃ³n injusta y tergiversaciÃ³n  
      
        - SÃ­
          - ğŸš¨ **Riesgo Alto** ğŸ”´  
          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Realizar auditorÃ­as para identificar quÃ© grupos dependen del sistema y asegurar alternativas de acceso antes de la retirada.  
          <br> ğŸ› ï¸ **Herramientas recomendadas:** 
          [Aequitas](https://github.com/dssg/aequitas)
      
        - No 
          - âœ… **Riesgo Bajo o Moderado** ğŸŸ¢  
          <br>De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de gÃ©nero. Mantenga auditorÃ­as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de anÃ¡lisis inclusivas para garantizar la equidad en el desarrollo o adquisiciÃ³n de su soluciÃ³n de IA.
          
          
      
    </script>

         <!-- Scripts necesarios -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/markmap-view/dist/markmap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.16"></script>

        <script src="js/arbol.js" defer></script>
        <script src="js/header.js" defer></script>
    </div>

  
</body>
</html>
