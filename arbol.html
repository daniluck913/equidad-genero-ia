<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>√Årbol de decisi√≥n</title>
    <!-- Estilos -->
    <link rel="stylesheet" href="css/styles.css">
    <link rel="stylesheet" href="css/arbol.css">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
</head>
  </head>
<body>
    <div class="container">
        <!-- Prop√≥sito de la herramienta -->
        <section id="tool-purpose" class="mb-4">
            <div class="rounded-container p-3">
                <button class="close-button" aria-label="Cerrar">&times;</button>
                <h2 id="page-title" class="mb-0"></h2>
                <p>
                  El √°rbol de decisi√≥n es una herramienta interactiva dise√±ada para ayudar a los actores del ecosistema de inteligencia artificial (IA) a identificar, categorizar y gestionar riesgos asociados con sesgos de g√©nero en las soluciones de IA. A partir de preguntas clave y respuestas espec√≠ficas, el √°rbol gu√≠a a los usuarios a trav√©s de un proceso l√≥gico y estructurado para tomar decisiones informadas, implementar acciones preventivas y utilizar herramientas adecuadas.
                </p>
                <p>
                    Los sesgos identificados han sido clasificados seg√∫n la Taxonom√≠a de Riesgos de IA del MIT, permitiendo su categorizaci√≥n dentro de un marco reconocido internacionalmente. Esta clasificaci√≥n facilita una mejor comprensi√≥n de los riesgos y su impacto en el desarrollo de IA m√°s equitativa y responsable. Para m√°s informaci√≥n, visita: <a href="https://airisk.mit.edu/" target="_blank">MIT AI Risk Repository</a>.
                </p>
            </div>
        </section>

    </div>
    
<div class="markmap" id="mindmap">
    <script type="text/template">

      ---
title: √Årbol de decisi√≥n
markmap:
  colorFreezeLevel: 3
  embedAssets: true
  maxWidth: 300
  initialExpandLevel: 2
  
---

# üå≥ √Årbol de Decisi√≥n

## üñ•Ô∏èü§ñ ¬øDesea desarrollar una soluci√≥n basada en IA? ¬øEn qu√© fase se encuentra?

### 1Ô∏è‚É£ üìú Planificaci√≥n y dise√±o  
- ¬øLas hip√≥tesis iniciales o las suposiciones del proyecto han sido revisadas cr√≠ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øEl dise√±o del sistema considera c√≥mo los usuarios podr√≠an interactuar de manera sesgada hacia ciertos g√©neros, y se han implementado medidas para mitigar estos patrones?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 02.08.00  
      **Entidad:** Humano  
      **Intenci√≥n:** No intencional  
      **Momento:** Pre-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
         - ¬øEl dise√±o de la interfaz o sistema contempla las necesidades y experiencias de todos los g√©neros, asegurando una experiencia inclusiva y equitativa?  
           <br>üîñ **Clasificaci√≥n riesgo MIT**  
           **ID Riesgo:** 05.01.00  
           **Entidad:** IA  
           **Intenci√≥n:** No intencional  
           **Momento:** Post-implementaci√≥n  
           **Tema:** Discriminaci√≥n y toxicidad  
           **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

           - S√≠  
             - ¬øEl sistema es igualmente accesible y √∫til para personas de todos los g√©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
               <br>üîñ **Clasificaci√≥n riesgo MIT**  
               **ID Riesgo:** 05.01.00  
               **Entidad:** IA  
               **Intenci√≥n:** No intencional  
               **Momento:** Post-implementaci√≥n  
               **Tema:** Discriminaci√≥n y toxicidad  
               **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

               - S√≠  
                 - ¬øSe han considerado las realidades de g√©nero del contexto en el que se aplica la soluci√≥n, asegurando que los resultados sean equitativos?  
                   <br>üîñ **Clasificaci√≥n riesgo MIT**  
                   **ID Riesgo:** 05.01.00  
                   **Entidad:** IA  
                   **Intenci√≥n:** No intencional  
                   **Momento:** Post-implementaci√≥n  
                   **Tema:** Discriminaci√≥n y toxicidad  
                   **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n 

                   - S√≠  
                     - ¬øEl modelo aborda adecuadamente las particularidades de g√©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos g√©neros?  
                       <br>üîñ **Clasificaci√≥n riesgo MIT**  
                       **ID Riesgo:** 05.01.00  
                       **Entidad:** IA  
                       **Intenci√≥n:** No intencional  
                       **Momento:** Post-implementaci√≥n  
                       **Tema:** Discriminaci√≥n y toxicidad  
                       **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n 

                       - S√≠  
                         - ¬øEl equipo de desarrollo es diverso en t√©rminos de g√©nero y perspectiva?  
                           <br>üîñ **Clasificaci√≥n riesgo MIT**  
                           **ID Riesgo:** 01.01.00  
                           **Entidad:** Humano  
                           **Intenci√≥n:** No intencional  
                           **Momento:** Otros  
                           **Tema:** Socioecon√≥mico y ambiental  
                           **Subtema:** Falla de gobernanza  

                            - S√≠  
                              - ¬øEl prop√≥sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos g√©neros?  
                                <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                **ID Riesgo:** 05.01.00  
                                **Entidad:** IA  
                                **Intenci√≥n:** No intencional  
                                **Momento:** Post-implementaci√≥n  
                                **Tema:** Discriminaci√≥n y toxicidad  
                                **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                - S√≠  
                                  - ¬øSe han definido responsabilidades claras en el equipo para prevenir sesgos de g√©nero?  
                                    <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                    **ID Riesgo:** 01.01.00  
                                    **Entidad:** Humano  
                                    **Intenci√≥n:** No intencional  
                                    **Momento:** Otros  
                                    **Tema:** Socioecon√≥mico y ambiental  
                                    **Subtema:** Falla de gobernanza  

                                    - S√≠
                                      - ‚úÖ **Riesgo Bajo** üü¢
                                      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
                                     <br>üõ†Ô∏è **Herramientas recomendadas:** [AI Fairness 360](https://aif360.readthedocs.io/en/stable/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/),  [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators), [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)

                                    - No
                                      - üö® **Riesgo Alto üî¥ 
                                      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda establecer mecanismos de rendici√≥n de cuentas, definir roles de responsabilidad en el equipo y aplicar auditor√≠as internas y externas sobre equidad de g√©nero.  
                                      <br>üõ†Ô∏è **Herramientas recomendadas:** [AI Fairness 360](https://aif360.readthedocs.io/en/stable/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                                      
                                - No  
                                  - üö® **Riesgo Alto** üî¥  
                                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda involucrar a expertos en g√©nero, comunidades locales y otros actores relevantes para identificar necesidades espec√≠ficas y c√≥mo el sistema puede impactar a diferentes g√©neros en el contexto.  

                            - No
                              - ‚ö†Ô∏è **Riesgo Medio** üü†  
                              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como medio. Se recomienda fomentar la diversidad en la conformaci√≥n del equipo de desarrollo, implementar pol√≠ticas de inclusi√≥n y realizar auditor√≠as de equidad para evitar sesgos en el dise√±o y desarrollo de la IA.  
 
                       - No
                         - üö® **Riesgo Alto** üî¥  
                        <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar c√≥mo el modelo aplica sus predicciones en el dominio espec√≠fico, ajustar par√°metros seg√∫n las necesidades.  
 
                   - No
                     - üö® **Riesgo Alto** üî¥  
                    <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de g√©nero del contexto. Para ello, eval√∫e la representatividad de los datos, aplique t√©cnicas de mitigaci√≥n de sesgos y valide el impacto en distintos grupos.  
 
               - No 
                 - üö® **Riesgo Alto** üî¥  
                  <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditor√≠as de accesibilidad desde una perspectiva de g√©nero, implementar mejoras en el dise√±o y garantizar est√°ndares de accesibilidad universal.  
                  <br>üõ†Ô∏è **Herramientas recomendadas:** [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  

           - No 
             - üö® **Riesgo Alto** üî¥  
              <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el dise√±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de g√©nero y ajustar elementos de dise√±o para garantizar neutralidad.  
              <br>üõ†Ô∏è **Herramientas recomendadas:** [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)  

      - No 
         - üö® **Riesgo Alto** üî¥  
          <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables.  
          <br>üõ†Ô∏è **Herramientas recomendadas:** [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)  

  - No
     - üö® **Riesgo Alto** üî¥  
      <br> De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cr√≠ticamente las hip√≥tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
      <br>üõ†Ô∏è **Herramientas recomendadas:** [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom), [UserTesting](https://www-usertesting-com.translate.goog/)  
  

### 2Ô∏è‚É£ üìÇ Recopilaci√≥n y tratamiento de datos  
- ¬øLos datos utilizados reflejan de manera equitativa y proporcional a todos los g√©neros, incluidos aquellos hist√≥ricamente subrepresentados o con identidades diversas?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øEl proceso de selecci√≥n de datos evita privilegiar estereotipos o caracter√≠sticas espec√≠ficas asociadas a ciertos g√©neros, y se basa en criterios inclusivos y objetivos?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 02.01.01  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Otros  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øSe han identificado y mitigado patrones o roles de g√©nero perpetuados en los datos que podr√≠an trasladar desigualdades pasadas al modelo?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 02.08.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Pre-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠  
            - ¬øSe han revisado los datos para garantizar que no excluyan representaciones importantes de g√©neros diversos o minoritarios?  
              <br>üîñ **Clasificaci√≥n riesgo MIT**  
              **ID Riesgo:** 02.08.02  
              **Entidad:** IA  
              **Intenci√≥n:** No intencional  
              **Momento:** Pre-implementaci√≥n  
              **Tema:** Discriminaci√≥n y toxicidad  
              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

              - S√≠  
                - ¬øLas hip√≥tesis iniciales o las suposiciones del proyecto han sido revisadas cr√≠ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                  **ID Riesgo:** 02.08.02  
                  **Entidad:** IA  
                  **Intenci√≥n:** No intencional  
                  **Momento:** Pre-implementaci√≥n  
                  **Tema:** Discriminaci√≥n y toxicidad  
                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                  - S√≠  
                    - ¬øSe est√°n buscando y considerando activamente evidencias que desaf√≠en suposiciones previas relacionadas con g√©nero, en lugar de reforzar estereotipos existentes?  
                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                      **ID Riesgo:** 02.08.02  
                      **Entidad:** IA  
                      **Intenci√≥n:** No intencional  
                      **Momento:** Pre-implementaci√≥n  
                      **Tema:** Discriminaci√≥n y toxicidad  
                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                      - S√≠  
                        - ¬øLa soluci√≥n garantiza que todas las personas, independientemente de su g√©nero, tengan igual acceso, participaci√≥n y representaci√≥n en las interacciones con el sistema?  
                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                          **ID Riesgo:** 05.01.00  
                          **Entidad:** IA  
                          **Intenci√≥n:** No intencional  
                          **Momento:** Post-implementaci√≥n  
                          **Tema:** Discriminaci√≥n y toxicidad  
                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                          - S√≠  
                            - ¬øSe han implementado medidas para evitar la filtraci√≥n de datos sensibles relacionados con g√©nero?  
                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                              **ID Riesgo:** 11.03.00  
                              **Entidad:** IA  
                              **Intenci√≥n:** No intencional  
                              **Momento:** Otros  
                              **Tema:** Seguridad y privacidad  
                              **Subtema:** Fugas de datos personales  

                              - S√≠  
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **bajo o moderado**. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.  
                                Se recomienda usar herramientas para mantener el riesgo bajo o moderado como:  
                                [Qualtrics](https://www.qualtrics.com/es/plataforma/), [UserTesting](https://www-usertesting-com.translate.goog/), [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/),  
                                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/),  
                                [Google Dataset Search](https://datasetsearch.research.google.com/help), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/),  
                                [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Aequitas](https://github.com/dssg/aequitas)  
                              - No  
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda realizar encuestas de satisfacci√≥n segmentadas por g√©nero, identificar barreras de acceso y mejorar el dise√±o para garantizar una experiencia inclusiva.  
                                Herramientas: [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
                          - No
                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacci√≥n segmentadas por g√©nero, identificar barreras de acceso y mejorar el dise√±o para garantizar una experiencia inclusiva. Herramientas: [Qualtrics](https://www.qualtrics.com/es/plataforma/), [UserTesting](https://www-usertesting-com.translate.goog/)
                      - No  
                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda aplicar t√©cnicas de anonimizaci√≥n de datos y privacidad diferencial para proteger informaci√≥n de identidad de g√©nero.  
                        Herramientas: [AI Fairness 360](https://aif360.readthedocs.io/en/stable/), [Aequitas](https://github.com/dssg/aequitas)  
                  - No  
                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda ampliar el an√°lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad.  
                      Herramientas: [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)  
              - No  
                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda revisar cr√≠ticamente las hip√≥tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  
                Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
          - No  
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de g√©nero y realizar auditor√≠as de inclusi√≥n.  
            Herramientas: [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Google Dataset Search](https://datasetsearch.research.google.com/help)  
      - No  
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda auditar los datos para identificar tendencias hist√≥ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual.  
        Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  
  - No  
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como **alto**. Se recomienda revisar y ajustar los criterios de selecci√≥n para garantizar neutralidad, as√≠ como auditar los datos seleccionados para identificar patrones sesgados.  
    Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)  

### 3Ô∏è‚É£ ‚öôÔ∏è Creaci√≥n de modelo(s) y/o adaptaci√≥n de modelo(s)
- ¬øLos datos utilizados reflejan de manera equitativa y proporcional a todos los g√©neros, incluidos aquellos hist√≥ricamente subrepresentados o con identidades diversas?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øEl proceso de selecci√≥n de datos evita privilegiar estereotipos o caracter√≠sticas espec√≠ficas asociadas a ciertos g√©neros, y se basa en criterios inclusivos y objetivos?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 02.01.01  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Otros  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øSe han identificado y mitigado patrones o roles de g√©nero perpetuados en los datos que podr√≠an trasladar desigualdades pasadas al modelo?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 02.08.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Pre-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠  
            - ¬øSe han revisado los datos para garantizar que no excluyan representaciones importantes de g√©neros diversos o minoritarios?  
              <br>üîñ **Clasificaci√≥n riesgo MIT**  
              **ID Riesgo:** 02.08.02  
              **Entidad:** IA  
              **Intenci√≥n:** No intencional  
              **Momento:** Pre-implementaci√≥n  
              **Tema:** Discriminaci√≥n y toxicidad  
              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

              - S√≠  
                - ¬øEl modelo ha sido evaluado en contextos espec√≠ficos relacionados con g√©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                  **ID Riesgo:** 05.01.00  
                  **Entidad:** IA  
                  **Intenci√≥n:** No intencional  
                  **Momento:** Post-implementaci√≥n  
                  **Tema:** Discriminaci√≥n y toxicidad  
                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                  - S√≠  
                    - ¬øSe valida que el modelo sea aplicable a contextos con din√°micas de g√©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                      **ID Riesgo:** 05.01.00  
                      **Entidad:** IA  
                      **Intenci√≥n:** No intencional  
                      **Momento:** Post-implementaci√≥n  
                      **Tema:** Discriminaci√≥n y toxicidad  
                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                      - S√≠  
                        - ¬øSe han considerado las realidades culturales y de g√©nero del contexto en el que se aplica la soluci√≥n, asegurando que los resultados sean culturalmente sensibles y equitativos?  
                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                          **ID Riesgo:** 05.01.00  
                          **Entidad:** IA  
                          **Intenci√≥n:** No intencional  
                          **Momento:** Post-implementaci√≥n  
                          **Tema:** Discriminaci√≥n y toxicidad  
                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                          - S√≠  
                            - ¬øEl modelo aborda adecuadamente las particularidades de g√©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos g√©neros?  
                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                              **ID Riesgo:** 05.01.00  
                              **Entidad:** IA  
                              **Intenci√≥n:** No intencional  
                              **Momento:** Post-implementaci√≥n  
                              **Tema:** Discriminaci√≥n y toxicidad  
                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                              - S√≠
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
                                Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: 
                                [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [Aequitas](https://github.com/dssg/aequitas), [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/), [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Google Dataset Search](https://datasetsearch.research.google.com/help)
                              - No 
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar c√≥mo el modelo aplica sus predicciones en el dominio espec√≠fico, ajustar par√°metros seg√∫n las necesidades. 
                          - No 
                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar an√°lisis cultural del modelo en el contexto espec√≠fico, colaborar con expertos locales y ajustar los resultados para reflejar las din√°micas culturales. 
                            Herramientas:  [AI Fairness 360](https://aif360.readthedocs.io/en/stable/),  [Fairlearn](https://github.com/markmap/coc-markmap)
                      - No
                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar pruebas de transferencia en el nuevo contexto, ajustar los par√°metros del modelo y validar que los resultados reflejen din√°micas de g√©nero locales. Herramientas:   [AI Fairness 360](https://aif360.readthedocs.io/en/stable/),  [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)
                  - No 
                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios espec√≠ficos de g√©nero, ajustar par√°metros para mejorar el desempe√±o en esos contextos y realizar validaciones peri√≥dicas. 
                    Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
              - No 
                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de g√©nero y realizar auditor√≠as de inclusi√≥n. 
                Herramientas:  [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Google Dataset Search](https://datasetsearch.research.google.com/help)
          - No 
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda auditar los datos para identificar tendencias hist√≥ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual. 
            Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
      - No
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar y ajustar los criterios de selecci√≥n para garantizar neutralidad, as√≠ como auditar los datos seleccionados para identificar patrones sesgados. Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/) 
  - No 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar un an√°lisis de distribuci√≥n de datos para identificar grupos subrepresentados, ampliar la recolecci√≥n de datos de fuentes diversas y aplicar t√©cnicas de balanceo o re-pesado para corregir desbalances en los datos. Herramientas:  [Fairlearn](https://github.com/markmap/coc-markmap),  [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/),  [Aequitas](https://github.com/dssg/aequitas)

### 4Ô∏è‚É£ üîç Prueba, evaluaci√≥n, verificaci√≥n y validaci√≥n
- ¬øLas hip√≥tesis iniciales o las suposiciones del proyecto han sido revisadas cr√≠ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øSe est√°n buscando y considerando activamente evidencias que desaf√≠en suposiciones previas relacionadas con g√©nero, en lugar de reforzar estereotipos existentes?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 02.08.02  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Pre-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øLos resultados del modelo est√°n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de g√©nero?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 05.01.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Post-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠  
            - ¬øEl dise√±o del sistema considera c√≥mo los usuarios podr√≠an interactuar de manera sesgada hacia ciertos g√©neros, y se han implementado medidas para mitigar estos patrones?  
              <br>üîñ **Clasificaci√≥n riesgo MIT**  
              **ID Riesgo:** 02.08.00  
              **Entidad:** Humano  
              **Intenci√≥n:** No intencional  
              **Momento:** Pre-implementaci√≥n  
              **Tema:** Discriminaci√≥n y toxicidad  
              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

              - S√≠  
                - ¬øLa soluci√≥n garantiza que todas las personas, independientemente de su g√©nero, tengan igual acceso, participaci√≥n y representaci√≥n en las interacciones con el sistema?  
                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                  **ID Riesgo:** 05.01.00  
                  **Entidad:** IA  
                  **Intenci√≥n:** No intencional  
                  **Momento:** Post-implementaci√≥n  
                  **Tema:** Discriminaci√≥n y toxicidad  
                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                  - S√≠  
                    - ¬øEl dise√±o de la interfaz o sistema contempla las necesidades y experiencias de todos los g√©neros, asegurando una experiencia inclusiva y equitativa?  
                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                      **ID Riesgo:** 05.01.00  
                      **Entidad:** IA  
                      **Intenci√≥n:** No intencional  
                      **Momento:** Post-implementaci√≥n  
                      **Tema:** Discriminaci√≥n y toxicidad  
                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                      - S√≠  
                        - ¬øEl sistema es igualmente accesible y √∫til para personas de todos los g√©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                          **ID Riesgo:** 05.01.00  
                          **Entidad:** IA  
                          **Intenci√≥n:** No intencional  
                          **Momento:** Post-implementaci√≥n  
                          **Tema:** Discriminaci√≥n y toxicidad  
                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                          - S√≠  
                            - ¬øEl modelo ha sido evaluado en contextos espec√≠ficos relacionados con g√©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                              **ID Riesgo:** 05.01.00  
                              **Entidad:** IA  
                              **Intenci√≥n:** No intencional  
                              **Momento:** Post-implementaci√≥n  
                              **Tema:** Discriminaci√≥n y toxicidad  
                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                              - S√≠  
                                - ¬øSe monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de g√©nero, y se aplican correcciones cuando es necesario?  
                                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                  **ID Riesgo:** 05.01.00  
                                  **Entidad:** IA  
                                  **Intenci√≥n:** No intencional  
                                  **Momento:** Post-implementaci√≥n  
                                  **Tema:** Discriminaci√≥n y toxicidad  
                                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                  - S√≠  
                                    - ¬øEl modelo evita amplificar desigualdades o estereotipos de g√©nero existentes en los datos o interacciones del sistema?  
                                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                      **ID Riesgo:** 05.01.00  
                                      **Entidad:** IA  
                                      **Intenci√≥n:** No intencional  
                                      **Momento:** Post-implementaci√≥n  
                                      **Tema:** Discriminaci√≥n y toxicidad  
                                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                      - S√≠  
                                        - ¬øLas adaptaciones del sistema consideran posibles sesgos de g√©nero en los datos y resultados, y se ajustan para evitar perpetuar errores o desigualdades?  
                                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                          **ID Riesgo:** 05.01.00  
                                          **Entidad:** IA  
                                          **Intenci√≥n:** No intencional  
                                          **Momento:** Post-implementaci√≥n  
                                          **Tema:** Discriminaci√≥n y toxicidad  
                                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                          - S√≠  
                                            - ¬øEl modelo ha sido probado en contextos donde pueda reforzar desigualdades de g√©nero?  
                                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                              **ID Riesgo:** 05.01.00  
                                              **Entidad:** IA  
                                              **Intenci√≥n:** No intencional  
                                              **Momento:** Post-implementaci√≥n  
                                              **Tema:** Discriminaci√≥n y toxicidad  
                                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  
  
                                              - S√≠ 
                                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
                                                Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html),                                        [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html),[What-If Tool](https://pair-code.github.io/what-if-tool/get-started/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators), [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)
                                              - No
                                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el desempe√±o del modelo en diferentes grupos poblacionales y aplicar t√©cnicas de mitigaci√≥n de sesgos.  
                                                Herramientas: [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)
                                          - No 
                                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los procesos de adaptaci√≥n autom√°tica del sistema, establecer reglas para detectar adaptaciones sesgadas y ajustar el modelo seg√∫n sea necesario. Herramientas: [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html)
                                      - No
                                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar las predicciones para identificar amplificaciones no deseadas, reentrenar el modelo si se detectan patrones sesgados y validar los resultados con datos ajustados. Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                                  - No 
                                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema peri√≥dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas. 
                                    Herramientas: [Aequitas](https://github.com/dssg/aequitas)
                              - No
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios espec√≠ficos de g√©nero, ajustar par√°metros para mejorar el desempe√±o en esos contextos y realizar validaciones peri√≥dicas. 
                                Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
                          - No
                            -  De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditor√≠as de accesibilidad desde una perspectiva de g√©nero, implementar mejoras en el dise√±o y garantizar est√°ndares de accesibilidad universal. Herramientas: [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)
                      - No 
                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el dise√±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de g√©nero y ajustar elementos de dise√±o para garantizar neutralidad. Herramientas: [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)
                  - No
                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacci√≥n segmentadas por g√©nero, identificar barreras de acceso y mejorar el dise√±o para garantizar una experiencia inclusiva. Herramientas: [Qualtrics](https://www.qualtrics.com/es/plataforma/), [UserTesting](https://www-usertesting-com.translate.goog/)
              - No 
                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables. 
                Herramientas: [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom), [UserTesting](https://www-usertesting-com.translate.goog/)  
          - No
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaci√≥n. 
            Herramientas:  [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)
      - No 
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el an√°lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad. Herramientas: [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/),  [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
  - No
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cr√≠ticamente las hip√≥tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones.  Herramientas:[Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)

### 5Ô∏è‚É£ üöÄ Entrada en servicio/despliegue  
- ¬øLa soluci√≥n garantiza que todas las personas, independientemente de su g√©nero, tengan igual acceso, participaci√≥n y representaci√≥n en las interacciones con el sistema?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 05.01.00  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Post-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øEl dise√±o de la interfaz o sistema contempla las necesidades y experiencias de todos los g√©neros, asegurando una experiencia inclusiva y equitativa?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 05.01.00  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Post-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øLas decisiones durante la implementaci√≥n consideran posibles impactos desiguales entre g√©neros, y se toman medidas para garantizar resultados equitativos?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 05.01.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Post-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
            Se recomienda usar herramientas para mantener el riesgo bajo o moderado como:  [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas), [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)
          - No
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaci√≥n con un enfoque inclusivo, realizar auditor√≠as t√©cnicas y √©ticas y garantizar la participaci√≥n de expertos en g√©nero. 
            Herramientas: [Aequitas](https://github.com/dssg/aequitas)
      - No
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el dise√±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de g√©nero y ajustar elementos de dise√±o para garantizar neutralidad. Herramientas:[UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)
  - No 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacci√≥n segmentadas por g√©nero, identificar barreras de acceso y mejorar el dise√±o para garantizar una experiencia inclusiva. Herramientas: [Qualtrics](https://www.qualtrics.com/es/plataforma/), [UserTesting](https://www-usertesting-com.translate.goog/)

### 6Ô∏è‚É£ üì° Explotaci√≥n y supervisi√≥n (Monitoreo y ajuste)
- ¬øSe monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de g√©nero, y se aplican correcciones cuando es necesario?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 05.01.00  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Post-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øSe han evaluado impactos imprevistos de la IA en la equidad de g√©nero antes de su implementaci√≥n?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 05.01.00  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Post-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  
  
      - S√≠
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
        Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [Aequitas](https://github.com/dssg/aequitas)
      - No
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar estudios de impacto en equidad de g√©nero antes de la implementaci√≥n y evaluar c√≥mo diferentes poblaciones interact√∫an con la IA.  
        Herramientas: [Aequitas](https://github.com/dssg/aequitas)
  - No
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema peri√≥dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas. 
    Herramientas: [Aequitas](https://github.com/dssg/aequitas)

### 7Ô∏è‚É£ üóëÔ∏è Retirada/desmantelamiento  
- ¬øLa retirada del sistema afecta el acceso a ciertos g√©neros o grupos subrepresentados?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 05.01.00  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Post-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠ 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Realizar auditor√≠as para identificar qu√© grupos dependen del sistema y asegurar alternativas de acceso antes de la retirada. 
    Herramientas:  [Aequitas](https://github.com/dssg/aequitas)    
  - No 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
    
    
## üí∞ü§ñ ¬øDesea adquirir una IA? ¬øEn qu√© fase se encuentra

### 1Ô∏è‚É£ üìú Planificaci√≥n y an√°lisis inicial  
- ¬øLos resultados del modelo est√°n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de g√©nero?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 05.01.00  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Post-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠ 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
    Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)
  - No
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaci√≥n. Herramientas:  [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)

### 2Ô∏è‚É£ üîç Evaluaci√≥n y selecci√≥n de soluciones  
- ¬øLos datos utilizados reflejan de manera equitativa y proporcional a todos los g√©neros, incluidos aquellos hist√≥ricamente subrepresentados o con identidades diversas?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øEl proceso de selecci√≥n de datos evita privilegiar estereotipos o caracter√≠sticas espec√≠ficas asociadas a ciertos g√©neros, y se basa en criterios inclusivos y objetivos?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 02.01.01  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Otros  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øSe han identificado y mitigado patrones o roles de g√©nero perpetuados en los datos que podr√≠an trasladar desigualdades pasadas al modelo?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 02.08.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Pre-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠  
            - ¬øSe han revisado los datos para garantizar que no excluyan representaciones importantes de g√©neros diversos o minoritarios?  
              <br>üîñ **Clasificaci√≥n riesgo MIT**  
              **ID Riesgo:** 02.08.02  
              **Entidad:** IA  
              **Intenci√≥n:** No intencional  
              **Momento:** Pre-implementaci√≥n  
              **Tema:** Discriminaci√≥n y toxicidad  
              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

              - S√≠  
                - ¬øLas hip√≥tesis iniciales o las suposiciones del proyecto han sido revisadas cr√≠ticamente para evitar que influyan de manera sesgada en los datos o las decisiones?  
                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                  **ID Riesgo:** 02.08.02  
                  **Entidad:** IA  
                  **Intenci√≥n:** No intencional  
                  **Momento:** Pre-implementaci√≥n  
                  **Tema:** Discriminaci√≥n y toxicidad  
                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                  - S√≠  
                    - ¬øSe est√°n buscando y considerando activamente evidencias que desaf√≠en suposiciones previas relacionadas con g√©nero, en lugar de reforzar estereotipos existentes?  
                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                      **ID Riesgo:** 02.08.02  
                      **Entidad:** IA  
                      **Intenci√≥n:** No intencional  
                      **Momento:** Pre-implementaci√≥n  
                      **Tema:** Discriminaci√≥n y toxicidad  
                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                      - S√≠  
                        - ¬øSe valida que el modelo sea aplicable a contextos con din√°micas de g√©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                          **ID Riesgo:** 05.01.00  
                          **Entidad:** IA  
                          **Intenci√≥n:** No intencional  
                          **Momento:** Post-implementaci√≥n  
                          **Tema:** Discriminaci√≥n y toxicidad  
                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                          - S√≠  
                            - ¬øSe han considerado las realidades de g√©nero del contexto en el que se aplica la soluci√≥n, asegurando que los resultados sean equitativos?  
                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                              **ID Riesgo:** 05.01.00  
                              **Entidad:** IA  
                              **Intenci√≥n:** No intencional  
                              **Momento:** Post-implementaci√≥n  
                              **Tema:** Discriminaci√≥n y toxicidad  
                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                              - S√≠  
                                - ¬øEl modelo aborda adecuadamente las particularidades de g√©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos g√©neros?  
                                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                  **ID Riesgo:** 05.01.00  
                                  **Entidad:** IA  
                                  **Intenci√≥n:** No intencional  
                                  **Momento:** Post-implementaci√≥n  
                                  **Tema:** Discriminaci√≥n y toxicidad  
                                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                  - S√≠  
                                    - ¬øEl prop√≥sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos g√©neros?  
                                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                      **ID Riesgo:** 05.01.00  
                                      **Entidad:** IA  
                                      **Intenci√≥n:** No intencional  
                                      **Momento:** Post-implementaci√≥n  
                                      **Tema:** Discriminaci√≥n y toxicidad  
                                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                      - S√≠  
                                        - ¬øSe han realizado auditor√≠as sobre el contenido generado para evitar sesgos de g√©nero?  
                                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                          **ID Riesgo:** 05.01.00  
                                          **Entidad:** IA  
                                          **Intenci√≥n:** No intencional  
                                          **Momento:** Post-implementaci√≥n  
                                          **Tema:** Discriminaci√≥n y toxicidad  
                                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                          - S√≠  
                                            - ¬øSe han implementado mecanismos de explicabilidad para identificar sesgos de g√©nero en la toma de decisiones?  
                                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                              **ID Riesgo:** 05.01.00  
                                              **Entidad:** IA  
                                              **Intenci√≥n:** No intencional  
                                              **Momento:** Post-implementaci√≥n  
                                              **Tema:** Discriminaci√≥n y toxicidad  
                                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                              - S√≠
                                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
                                                Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [AI Fairness 360](https://aif360.readthedocs.io/en/stable/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [Aequitas](https://github.com/dssg/aequitas), [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/), [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Google Dataset Search](https://datasetsearch.research.google.com/help)
                                              - No
                                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que el sistema proporcione explicaciones claras sobre sus decisiones y utilizar t√©cnicas de interpretabilidad para detectar sesgos ocultos.  
                                                Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/), [SHAP](https://shap.readthedocs.io/en/latest/), [LIME](https://github.com/marcotcr/lime)
                                          - No
                                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda implementar revisiones sistem√°ticas del contenido generado por la IA y establecer filtros que detecten y mitiguen sesgos en el lenguaje.  
                                              Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/) 
                                      - No 
                                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Involucrar a expertos en g√©nero, comunidades locales y otros actores relevantes para identificar necesidades espec√≠ficas y c√≥mo el sistema puede impactar a diferentes g√©neros en el contexto.
                                  - No 
                                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar c√≥mo el modelo aplica sus predicciones en el dominio espec√≠fico, ajustar par√°metros seg√∫n las necesidades. 
                              - No 
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de g√©nero del contexto. Para ello, eval√∫e la representatividad de los datos, aplique t√©cnicas de mitigaci√≥n de sesgos y valide el impacto en distintos grupos. 
                          - No
                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de g√©nero del contexto. Para ello, eval√∫e la representatividad de los datos, aplique t√©cnicas de mitigaci√≥n de sesgos y valide el impacto en distintos grupos. 
                                Herramientas:  [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
                      - No 
                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el an√°lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad. Herramientas: [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/),  [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                  - No 
                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar cr√≠ticamente las hip√≥tesis iniciales con un panel diverso, realizar validaciones externas e incluir perspectivas inclusivas en las decisiones. Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)

              - No
                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar los datos en busca de exclusiones intencionales o accidentales, incluir datos adicionales que reflejen mejor la diversidad de g√©nero y realizar auditor√≠as de inclusi√≥n. Herramientas:  [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/), [Google Dataset Search](https://datasetsearch.research.google.com/help)
          - No 
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda auditar los datos para identificar tendencias hist√≥ricas sesgadas, reentrenar el modelo con datos actuales o ajustados y validar que los resultados reflejen equidad actual. 
            Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
      - No 
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar y ajustar los criterios de selecci√≥n para garantizar neutralidad, as√≠ como auditar los datos seleccionados para identificar patrones sesgados. Herramientas: [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html), [AI Fairness 360](https://aif360.readthedocs.io/en/stable/)
  - No
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar un an√°lisis de distribuci√≥n de datos para identificar grupos subrepresentados, ampliar la recolecci√≥n de datos de fuentes diversas y aplicar t√©cnicas de balanceo o re-pesado para corregir desbalances en los datos. Herramientas:  [Fairlearn](https://github.com/markmap/coc-markmap),  [Pandas Profiling](https://www.geeksforgeeks.org/pandas-profiling-in-python/),  [Aequitas](https://github.com/dssg/aequitas)

### 3Ô∏è‚É£ üöÄ Implementaci√≥n e integraci√≥n  
- ¬øEl dise√±o del sistema considera c√≥mo los usuarios podr√≠an interactuar de manera sesgada hacia ciertos g√©neros, y se han implementado medidas para mitigar estos patrones?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.00  
  **Entidad:** Humano  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øLas decisiones durante la implementaci√≥n consideran posibles impactos desiguales entre g√©neros, y se toman medidas para garantizar resultados equitativos?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 05.01.00  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Post-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
        Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)
      - No 
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaci√≥n con un enfoque inclusivo, realizar auditor√≠as t√©cnicas y √©ticas y garantizar la participaci√≥n de expertos en g√©nero. 
        Herramientas: [Aequitas](https://github.com/dssg/aequitas)
  - No 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear las interacciones de los usuarios en tiempo real, ajustar las recomendaciones y resultados basados en patrones de uso y sensibilizar a los usuarios sobre interacciones responsables.
    Herramientas: [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom), [UserTesting](https://www-usertesting-com.translate.goog/)  

### 4Ô∏è‚É£ üì° Explotaci√≥n y supervisi√≥n  
- ¬øSe est√°n buscando y considerando activamente evidencias que desaf√≠en suposiciones previas relacionadas con g√©nero, en lugar de reforzar estereotipos existentes?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 02.08.02  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Pre-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠  
    - ¬øLos resultados del modelo est√°n siendo interpretados de manera que eviten perpetuar estereotipos o ideas preconcebidas sobre roles de g√©nero?  
      <br>üîñ **Clasificaci√≥n riesgo MIT**  
      **ID Riesgo:** 05.01.00  
      **Entidad:** IA  
      **Intenci√≥n:** No intencional  
      **Momento:** Post-implementaci√≥n  
      **Tema:** Discriminaci√≥n y toxicidad  
      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

      - S√≠  
        - ¬øLa soluci√≥n garantiza que todas las personas, independientemente de su g√©nero, tengan igual acceso, participaci√≥n y representaci√≥n en las interacciones con el sistema?  
          <br>üîñ **Clasificaci√≥n riesgo MIT**  
          **ID Riesgo:** 05.01.00  
          **Entidad:** IA  
          **Intenci√≥n:** No intencional  
          **Momento:** Post-implementaci√≥n  
          **Tema:** Discriminaci√≥n y toxicidad  
          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

          - S√≠  
            - ¬øEl dise√±o de la interfaz o sistema contempla las necesidades y experiencias de todos los g√©neros, asegurando una experiencia inclusiva y equitativa?  
              <br>üîñ **Clasificaci√≥n riesgo MIT**  
              **ID Riesgo:** 05.01.00  
              **Entidad:** IA  
              **Intenci√≥n:** No intencional  
              **Momento:** Post-implementaci√≥n  
              **Tema:** Discriminaci√≥n y toxicidad  
              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

              - S√≠  
                - ¬øEl sistema es igualmente accesible y √∫til para personas de todos los g√©neros, incluidas aquellas con identidades no binarias o en contextos diversos?  
                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                  **ID Riesgo:** 05.01.00  
                  **Entidad:** IA  
                  **Intenci√≥n:** No intencional  
                  **Momento:** Post-implementaci√≥n  
                  **Tema:** Discriminaci√≥n y toxicidad  
                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                  - S√≠  
                    - ¬øEl modelo ha sido evaluado en contextos espec√≠ficos relacionados con g√©nero para garantizar que cumple con las expectativas sin generar resultados sesgados?  
                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                      **ID Riesgo:** 05.01.00  
                      **Entidad:** IA  
                      **Intenci√≥n:** No intencional  
                      **Momento:** Post-implementaci√≥n  
                      **Tema:** Discriminaci√≥n y toxicidad  
                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                      - S√≠  
                        - ¬øLas decisiones durante la implementaci√≥n consideran posibles impactos desiguales entre g√©neros, y se toman medidas para garantizar resultados equitativos?  
                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                          **ID Riesgo:** 05.01.00  
                          **Entidad:** IA  
                          **Intenci√≥n:** No intencional  
                          **Momento:** Post-implementaci√≥n  
                          **Tema:** Discriminaci√≥n y toxicidad  
                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                          - S√≠  
                            - ¬øSe monitorean las salidas del sistema para identificar si refuerzan patrones sesgados de g√©nero, y se aplican correcciones cuando es necesario?  
                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                              **ID Riesgo:** 05.01.00  
                              **Entidad:** IA  
                              **Intenci√≥n:** No intencional  
                              **Momento:** Post-implementaci√≥n  
                              **Tema:** Discriminaci√≥n y toxicidad  
                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                              - S√≠  
                                - ¬øEl modelo evita amplificar desigualdades o estereotipos de g√©nero existentes en los datos o interacciones del sistema?  
                                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                  **ID Riesgo:** 05.01.00  
                                  **Entidad:** IA  
                                  **Intenci√≥n:** No intencional  
                                  **Momento:** Post-implementaci√≥n  
                                  **Tema:** Discriminaci√≥n y toxicidad  
                                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                  - S√≠  
                                    - ¬øLas adaptaciones del sistema consideran posibles sesgos de g√©nero en los datos y resultados, y se ajustan para evitar perpetuar errores o desigualdades?  
                                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                      **ID Riesgo:** 05.01.00  
                                      **Entidad:** IA  
                                      **Intenci√≥n:** No intencional  
                                      **Momento:** Post-implementaci√≥n  
                                      **Tema:** Discriminaci√≥n y toxicidad  
                                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                      - S√≠  
                                        - ¬øSe valida que el modelo sea aplicable a contextos con din√°micas de g√©nero diferentes al original, y se ajusta para reflejar esas diferencias?  
                                          <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                          **ID Riesgo:** 05.01.00  
                                          **Entidad:** IA  
                                          **Intenci√≥n:** No intencional  
                                          **Momento:** Post-implementaci√≥n  
                                          **Tema:** Discriminaci√≥n y toxicidad  
                                          **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                          - S√≠  
                                            - ¬øSe han considerado las realidades de g√©nero del contexto en el que se aplica la soluci√≥n, asegurando que los resultados sean equitativos?  
                                              <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                              **ID Riesgo:** 05.01.00  
                                              **Entidad:** IA  
                                              **Intenci√≥n:** No intencional  
                                              **Momento:** Post-implementaci√≥n  
                                              **Tema:** Discriminaci√≥n y toxicidad  
                                              **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                              - S√≠  
                                                - ¬øEl modelo aborda adecuadamente las particularidades de g√©nero del dominio en el que se aplica, evitando generalizaciones que puedan afectar a ciertos g√©neros?  
                                                  <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                                  **ID Riesgo:** 05.01.00  
                                                  **Entidad:** IA  
                                                  **Intenci√≥n:** No intencional  
                                                  **Momento:** Post-implementaci√≥n  
                                                  **Tema:** Discriminaci√≥n y toxicidad  
                                                  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                                  - S√≠  
                                                    - ¬øEl prop√≥sito del modelo se ha evaluado para garantizar que no genere impactos desproporcionados o perjudiciales para ciertos g√©neros?  
                                                      <br>üîñ **Clasificaci√≥n riesgo MIT**  
                                                      **ID Riesgo:** 05.01.00  
                                                      **Entidad:** IA  
                                                      **Intenci√≥n:** No intencional  
                                                      **Momento:** Post-implementaci√≥n  
                                                      **Tema:** Discriminaci√≥n y toxicidad  
                                                      **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

                                                      - S√≠ 
                                                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
                                                        Se recomienda usar herramientas para mantener el riesgo bajo o moderado como: [AI Fairness 360](https://aif360.readthedocs.io/en/stable/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html),  [Aequitas](https://github.com/dssg/aequitas), [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html), [MLOps dashboards](https://github.com/SumoLogic-Labs/MLOPs-Dashboards), [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)
                                                      - No 
                                                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Involucrar a expertos en g√©nero, comunidades locales y otros actores relevantes para identificar necesidades espec√≠ficas y c√≥mo el sistema puede impactar a diferentes g√©neros en el contexto.   
                                                  - No  
                                                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar c√≥mo el modelo aplica sus predicciones en el dominio espec√≠fico, ajustar par√°metros seg√∫n las necesidades. 
                                              - No
                                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda asegurar que los resultados sean equitativos considerando las realidades de g√©nero del contexto. Para ello, eval√∫e la representatividad de los datos, aplique t√©cnicas de mitigaci√≥n de sesgos y valide el impacto en distintos grupos. 
                                          - No 
                                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar pruebas de transferencia en el nuevo contexto, ajustar los par√°metros del modelo y validar que los resultados reflejen din√°micas de g√©nero locales.
                                      - No 
                                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los procesos de adaptaci√≥n autom√°tica del sistema, establecer reglas para detectar adaptaciones sesgadas y ajustar el modelo seg√∫n sea necesario. Herramientas: [Google AutoML](https://www-run-ai.translate.goog/guides/automl/google-automl?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [herramientas de MLOps](https://www.purestorage.com/knowledge/mlops-tools.html) 
                                  - No 
                                    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda analizar las predicciones para identificar amplificaciones no deseadas, reentrenar el modelo si se detectan patrones sesgados y validar los resultados con datos ajustados. Herramientas: [What-If Tool](https://pair-code.github.io/what-if-tool/get-started/), [Fairlearn](https://fairlearn.org/v0.11/api_reference/index.html)
                              - No 
                                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda monitorear los resultados del sistema peri√≥dicamente, identificar patrones sesgados y ajustar el modelo para corregir retroalimentaciones negativas.
                                Herramientas: [Aequitas](https://github.com/dssg/aequitas)
                          - No
                            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar las decisiones de implementaci√≥n con un enfoque inclusivo, realizar auditor√≠as t√©cnicas y √©ticas y garantizar la participaci√≥n de expertos en g√©nero. 
                            Herramientas: [Aequitas](https://github.com/dssg/aequitas)
                      - No
                        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda probar el modelo en escenarios espec√≠ficos de g√©nero, ajustar par√°metros para mejorar el desempe√±o en esos contextos y realizar validaciones peri√≥dicas. 
                        Herramientas:[What-If Tool](https://pair-code.github.io/what-if-tool/get-started/)
                  - No 
                    -  De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar auditor√≠as de accesibilidad desde una perspectiva de g√©nero, implementar mejoras en el dise√±o y garantizar est√°ndares de accesibilidad universal. 
                    Herramientas: [Aequitas](https://github.com/dssg/aequitas), [Fairness Indicators](https://github.com/tensorflow/fairness-indicators)
              - No 
                - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda evaluar el dise√±o con grupos diversos, realizar pruebas de usabilidad enfocadas en equidad de g√©nero y ajustar elementos de dise√±o para garantizar neutralidad. Herramientas: [UserZoom](https://www-usertesting-com.translate.goog/platform/userzoom?_x_tr_sl=en&_x_tr_tl=es&_x_tr_hl=es-419&_x_tr_pto=sc), [UserTesting](https://www-usertesting-com.translate.goog/)
          - No 
            - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda realizar encuestas de satisfacci√≥n segmentadas por g√©nero, identificar barreras de acceso y mejorar el dise√±o para garantizar una experiencia inclusiva. Herramientas: [Qualtrics](https://www.qualtrics.com/es/plataforma/), [UserTesting](https://www-usertesting-com.translate.goog/)
      - No 
        - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda revisar los resultados con observadores externos, usar explicabilidad para justificar decisiones de IA y detectar posibles sesgos en la interpretaci√≥n. Herramientas:   [Fairlearn](https://github.com/markmap/coc-markmap), [Aequitas](https://github.com/dssg/aequitas)    
  - No
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Se recomienda ampliar el an√°lisis a contextos y escenarios que cuestionen las creencias previas, usar datos contrafactuales y validar los resultados para garantizar la neutralidad. 
    Herramientas: [What-If Tool (Google)](https://pair-code.github.io/what-if-tool/get-started/)

### 5Ô∏è‚É£ üóëÔ∏è Retirada o reemplazo de la soluci√≥n  
- ¬øLa retirada del sistema afecta el acceso a ciertos g√©neros o grupos subrepresentados?  
  <br>üîñ **Clasificaci√≥n riesgo MIT**  
  **ID Riesgo:** 05.01.00  
  **Entidad:** IA  
  **Intenci√≥n:** No intencional  
  **Momento:** Post-implementaci√≥n  
  **Tema:** Discriminaci√≥n y toxicidad  
  **Subtema:** Discriminaci√≥n injusta y tergiversaci√≥n  

  - S√≠
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como alto. Realizar auditor√≠as para identificar qu√© grupos dependen del sistema y asegurar alternativas de acceso antes de la retirada. 
    Herramientas:  [Aequitas](https://github.com/dssg/aequitas)
  - No 
    - De acuerdo con las opciones seleccionadas, su nivel de riesgo se clasifica como bajo o moderado. Se recomienda continuar implementando acciones proactivas para identificar, prevenir y mitigar posibles sesgos de g√©nero. Mantenga auditor√≠as regulares, fomente la diversidad en su equipo de trabajo y utilice herramientas de an√°lisis inclusivas para garantizar la equidad en el desarrollo o adquisici√≥n de su soluci√≥n de IA.
    
    

             
    </script>

         <!-- Scripts necesarios -->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/markmap-view/dist/markmap.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/markmap-autoloader@0.16"></script>

        <script src="js/arbol.js" defer></script>
        <script src="js/header.js" defer></script>
    </div>

  
</body>
</html>
